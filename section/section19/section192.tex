\section{大数定律}

大数定律是概率论中的一个基本定理, 描述了随机变量序列的均值在样本数量足够大的情况下会收敛于其数学期望的概率性现象. 大数定律是概率论和统计学中的重要定理之一, 对于理解随机现象的规律和稳定性具有重要意义.

\subsection{Bernoulli 大数定律}

\begin{theorem}[Bernoulli 大数定律]
    \index{Bernoulli 大数定律}
    设 $ n_{A} $ 是 $ n $ 重 Bernoulli 试验中事件 $ A $ 发生的次数, $ p~(0<p<1)$ 是事件 $ A $ 在一次试验中发生的概率, 则对任意给定的正数 $ \varepsilon $, 有
    $$\lim _{n \rightarrow \infty} P\left\{\left|\frac{n_{A}}{n}-p\right|<\varepsilon\right\}=1 .$$
\end{theorem}

\subsection{Chebyshev 大数定律}

\begin{theorem}[Chebyshev 大数定律]
    \index{Chebyshev 大数定律}
    设 $ X_{1}, X_{2}, \cdots, X_{n}, \cdots $ 是相互独立的随机变量序列, 其数学期望与方差都存在, 且方差一致有界, 即存在正数 $ M $, 对任意 $ i~(i=1,2, \cdots) $, 有
    $$D\left(X_{i}\right) \leqslant M$$
    则对任意给定的正数 $ \varepsilon $, 恒有
    $$\lim _{n \rightarrow \infty} P\left\{\left|\frac{1}{n} \sum_{i=1}^{n} X_{i}-\frac{1}{n} \sum_{i=1}^{n} E\left(X_{i}\right)\right|>\varepsilon\right\}=0 .$$
\end{theorem}

这一定理说明, 经过算术平均后得到的随机变量 $\displaystyle \bar{X}=\frac{1}{n} \sum_{i=1}^{n} X_{i} $ 在统计上具有一种稳定性, 它的取值将比较紧密地聚集在其期望附近. 这正是大数定律的含义, 在概率论中, 大数定律是随机现象的统计稳定性的深刻描述, 同时也是数理统计的重要理论基础.

Bernoulli 大数定律是 Chebyshev 大数定律的特殊情况, 即随机变量序列 $\displaystyle X_{i} \sim b(1, p) ,  \sum_{i=1}^{n} X_{i} $ 是 $ n $ 次独立重复试验中 $ A $ 发生的次数 $ n_{A} $, 即 $\displaystyle \bar{X}=\frac{1}{n} \sum_{i=1}^{n} X_{i} $ 为事件 $ A $ 发生的频率 $\displaystyle \frac{n_{A}}{n} $, 而 $\displaystyle \frac{1}{n} \sum_{i=1}^{n} E\left(X_{i}\right)=p $.

\begin{example}
    假设随机变量 $X_1, X_2, \cdots $ 相互独立, 且服从同参数 $\lambda$ 的泊松分布, 且下列随机变量序列中不满足 Chebyshev 大数定律条件的是 (\quad).
    \begin{tasks}(2)
        \task $X_1, X_2, \cdots ,X_n, \cdots $
        \task $X_1+1, X_2+2, \cdots ,X_n+n, \cdots $
        \task $X_1, 2X_2, \cdots ,nX_n, \cdots $
        \task $X_1, \dfrac{1}{2}X_2, \cdots ,\dfrac{1}{n}X_n, \cdots $
    \end{tasks}
\end{example}
\begin{solution}
    \begin{enumerate}[label=(\arabic{*})]
        \item 判断是否相互独立,
              由题意知 $$X_1, X_2, \cdots ,X_n, \cdots ; X_1+1, X_2+2, \cdots ,X_n+n, \cdots ; X_1, 2X_2, \cdots ,nX_n, \cdots ; X_1, \dfrac{1}{2}X_2, \cdots ,\dfrac{1}{n}X_n, \cdots $$
              均相互独立;
        \item 判断随机变量的期望方差是否均存在,
        \begin{math}
            EX_n=\lambda, DX_n=\lambda, E(X_n+n)=\lambda+n, D(X_n+n)=\lambda,\\ 
            E(nX_n)=n\lambda, D(nX_n)=n^2\lambda, E\qty(\dfrac{1}{n}X_n)=\dfrac{\lambda}{n},D\qty(\dfrac{1}{n}X_n)=\dfrac{\lambda}{n^2}
        \end{math}
        \item 判断方差是否有公共上界, 
        对于 (A), 有 $DX_n=\lambda<\lambda+1$, 对于 (B), 有 $D(X_n+n)=\lambda<\lambda+1$, 对于 (C), 有 $D(nX_n)=n^2\lambda$ 没有公共上界, 对于 (D), 有 $D\qty(\dfrac{1}{n}X_n)=\dfrac{\lambda}{n^2}<\lambda+1$, 综上, 选 C.
    \end{enumerate}
\end{solution}

\subsection{Khinchin 大数定律}

\begin{theorem}[Khinchin 大数定律]
    \index{Khinchin 大数定律}
    \label{Khinchinslawoflargenumbers}
    设随机变量序列 $ X_{1}, X_{2}, \cdots, X_{n}, \cdots $ 相互独立且服从相同的分布, 具有数学期望 $ E\left(X_{i}\right)=\mu, i=1,2, \cdots $, 则对任意给定的正数 $ \varepsilon $, 有
    $$\lim _{n \rightarrow \infty} P\left\{\left|\frac{1}{n} \sum_{i=1}^{n} X_{i}-\mu\right|<\varepsilon\right\}=1 .$$
\end{theorem}

Khinchin 大数定律中对 $ D\left(X_{i}\right) $ 不再有要求, 即不用再验证方差 $ D\left(X_{i}\right) $ 是否存在.
因此, 它比 Chebyshev 大数定律使用更方便.

\begin{example}
    设随机序列变量 $X_1, X_2, \cdots ,X_n, \cdots $ 独立同分布, 且 $X_1$ 的概率密度为 $$f(x)=\begin{cases}
            1-|x|, & |x|<1 \\ 0,&\text{其他}
        \end{cases}$$ 则当 $n\to \infty$ 时, $\displaystyle \dfrac{1}{n}\sum_{i=1}^{n} X_i^2$ 依概率收敛于 (\quad).
    \begin{tasks}(4)
        \task $\dfrac{1}{8}$.
        \task $\dfrac{1}{6}$.
        \task $\dfrac{1}{3}$.
        \task $\dfrac{1}{2}$.
    \end{tasks}
\end{example}
\begin{solution}
    由于随机变量序列  $X_1, X_2, \cdots ,X_n, \cdots $ 独立同分布, 那么  $X_1^2, X_2^2, \cdots ,X_n^2, \cdots $ 也独立同分布,
    根据定理 \ref{Khinchinslawoflargenumbers} 知 $\displaystyle \dfrac{1}{n}\sum_{i=1}^{n} X_i^2$ 依概率收敛于 $E\qty(X_1^2)$,
    $$
        E\qty(X_1^2)=\int_{-1}^{1} x^2(1-|x|) \dd x=\dfrac{1}{6}
    $$
    因此选 B.
\end{solution}